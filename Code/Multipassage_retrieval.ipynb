{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMJXhA18olmhhnLe9/DqEVV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joykw1/NLP_RAG_project/blob/main/Code/Multipassage_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaXKxylLgzvI"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers bitsandbytes accelerate outlines datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "checkpoint = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "\n",
        "# Configure 8-bit quantization. We use this to save VRAM, as we don't have a lot available.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True  # Enables 8-bit quantization\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    checkpoint,\n",
        "    quantization_config=bnb_config,  # Apply BitsAndBytesConfig\n",
        "    device_map=\"cuda\"   # Assign to GPU\n",
        ")"
      ],
      "metadata": {
        "id": "M7ZT3qT6lpCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ru_dataset = load_dataset(\"xquad\", \"xquad.ru\")\n",
        "en_dataset = load_dataset(\"xquad\", \"xquad.en\")\n",
        "de_dataset = load_dataset(\"xquad\", \"xquad.de\")"
      ],
      "metadata": {
        "id": "lhTq_9Twphli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dict = {\n",
        "    'ru': ru_dataset,\n",
        "    'en': en_dataset,\n",
        "    'de': de_dataset\n",
        "}"
      ],
      "metadata": {
        "id": "W26sppKurpSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_dataset"
      ],
      "metadata": {
        "id": "-J5Ge4eHq0bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "\n",
        "# Define the configurations\n",
        "CONFIGS = {\n",
        "    # Original setup: Russian question, all contexts\n",
        "    \"ru_all\": {\n",
        "        \"question_lang\": \"ru\",\n",
        "        \"context_langs\": [\"ru\", \"en\", \"de\"],\n",
        "        \"answer_lang\": \"ru\",\n",
        "        \"base_dataset\": \"ru\"\n",
        "    },\n",
        "    # Setup 1: Russian question, only English and German contexts\n",
        "    \"ru_en_de\": {\n",
        "        \"question_lang\": \"ru\",\n",
        "        \"context_langs\": [\"en\", \"de\"],\n",
        "        \"answer_lang\": \"ru\",\n",
        "        \"base_dataset\": \"ru\"\n",
        "    },\n",
        "    # Setup 2: English question, Russian and German contexts\n",
        "    \"en_ru_de\": {\n",
        "        \"question_lang\": \"en\",\n",
        "        \"context_langs\": [\"ru\", \"de\"],\n",
        "        \"answer_lang\": \"en\",\n",
        "        \"base_dataset\": \"en\"\n",
        "    },\n",
        "    # Setup 3: German question, all contexts\n",
        "    \"de_all\": {\n",
        "        \"question_lang\": \"de\",\n",
        "        \"context_langs\": [\"ru\", \"de\", \"en\"],\n",
        "        \"answer_lang\": \"de\",\n",
        "        \"base_dataset\": \"de\"\n",
        "    },\n",
        "    # Setup 4: German question, English and Russian contexts\n",
        "    \"de_ru_en\": {\n",
        "        \"question_lang\": \"de\",\n",
        "        \"context_langs\": [\"ru\", \"en\"],\n",
        "        \"answer_lang\": \"de\",\n",
        "        \"base_dataset\": \"de\"\n",
        "    },\n",
        "    # Setup 5: English question, all contexts\n",
        "    \"en_all\": {\n",
        "        \"question_lang\": \"en\",\n",
        "        \"context_langs\": [\"de\", \"ru\", \"en\"],\n",
        "        \"answer_lang\": \"en\",\n",
        "        \"base_dataset\": \"en\"\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "# Choose which configuration to run\n",
        "# Options: 'ru_all', 'ru_en_de', 'en_ru_de',\n",
        "config_name = 'de_ru_en'  # Change this to run different configurations\n",
        "config = CONFIGS[config_name]\n",
        "\n",
        "print(f\"Running with configuration: {config_name}\")\n",
        "print(f\"Question language: {config['question_lang']}\")\n",
        "print(f\"Context languages: {config['context_langs']}\")\n",
        "print(f\"Answer language: {config['answer_lang']}\")\n",
        "\n",
        "# Parameters\n",
        "batch_size = 10        # How often to save to CSV\n",
        "max_samples = None     # Set to a number to limit processing (e.g., 10 for testing)\n",
        "\n",
        "\n",
        "# Create output file paths\n",
        "output_path = f\"multilingual_qa_results_{config_name}.jsonl\"\n",
        "output_csv = f\"multilingual_qa_results_{config_name}.csv\"\n",
        "\n",
        "# Check if the output file already exists to determine where to start\n",
        "existing_ids = set()\n",
        "if os.path.exists(output_path):\n",
        "    with open(output_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                data = json.loads(line.strip())\n",
        "                existing_ids.add(data.get('id'))\n",
        "            except:\n",
        "                continue\n",
        "    print(f\"Found {len(existing_ids)} existing processed examples\")\n",
        "\n",
        "# Function to create a chat template based on the configuration\n",
        "def create_chat_input(question, contexts, config):\n",
        "    # Get configuration parameters\n",
        "    question_lang = config[\"question_lang\"]\n",
        "    context_langs = config[\"context_langs\"]\n",
        "\n",
        "    # Map language codes to full names\n",
        "    lang_names = {\"ru\": \"Russian\", \"en\": \"English\", \"de\": \"German\"}\n",
        "\n",
        "    # Build the contexts section of the prompt\n",
        "    contexts_text = \"\"\n",
        "    for lang in context_langs:\n",
        "        lang_name = lang_names[lang]\n",
        "        context = contexts[lang]\n",
        "        contexts_text += f\"{lang_name} Context: {context}\\n\\n\"\n",
        "\n",
        "    # Get the name of the question language\n",
        "    question_lang_name = lang_names[question_lang]\n",
        "\n",
        "    # Create the prompt with appropriate language instruction\n",
        "    prompt = f\"\"\"\n",
        "I need help with a question answering task using multiple languages.\n",
        "\n",
        "{contexts_text}Question ({question_lang_name}): {question}\n",
        "\n",
        "Your answer must be in {question_lang_name}.\n",
        "Your answer must contain only words from the contexts.\n",
        "Your answer must be a single noun phrase.\n",
        "If the question is 'how many' your answer must be a single numeral.\n",
        "If the question is 'who' your answer should only contain names or nouns.\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# Function to generate an answer\n",
        "def generate_answer(input_text):\n",
        "    # Apply the model's chat template\n",
        "    if tokenizer.chat_template:\n",
        "        messages = [{\"role\": \"user\", \"content\": input_text}]\n",
        "        formatted_input = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    else:\n",
        "        # Simple fallback if no chat template\n",
        "        formatted_input = f\"<|im_start|>user\\n{input_text}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "    # Tokenize and generate\n",
        "    inputs = tokenizer(formatted_input, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    response = response.strip()\n",
        "\n",
        "    # Clean up role prefixes that might appear in the output\n",
        "    # Remove \"user\" or \"system\" prefixes that might appear at the beginning\n",
        "    response = re.sub(r'^(user|system)\\s+', '', response)\n",
        "\n",
        "    # Clean up any remaining instances within the text\n",
        "    response = re.sub(r'\\n(user|system)\\s+', '\\n', response)\n",
        "\n",
        "    return response\n",
        "\n",
        "# Select the base dataset based on the configuration\n",
        "base_dataset = dataset_dict[config['base_dataset']]\n",
        "base_split = 'validation'  # We always use the validation split\n",
        "\n",
        "# Process each example and save results incrementally\n",
        "results = []\n",
        "total_examples = len(base_dataset[base_split])\n",
        "if max_samples is not None:\n",
        "    total_examples = min(total_examples, max_samples)\n",
        "\n",
        "print(f\"Processing {total_examples} examples...\")\n",
        "\n",
        "for idx, example in tqdm(enumerate(base_dataset[base_split]), total=total_examples):\n",
        "    if max_samples is not None and idx >= max_samples:\n",
        "        break\n",
        "\n",
        "    example_id = example['id']\n",
        "\n",
        "    # Skip if already processed\n",
        "    if example_id in existing_ids:\n",
        "        print(f\"Skipping already processed example ID: {example_id}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Process the example based on the configuration\n",
        "        question = example['question']\n",
        "\n",
        "        # Build contexts dictionary with aligned examples\n",
        "        contexts = {}\n",
        "        for lang in config[\"context_langs\"]:\n",
        "            if lang == config['base_dataset']:\n",
        "                # Use the context from the base dataset\n",
        "                contexts[lang] = example['context']\n",
        "            else:\n",
        "                # Find the same example in the other dataset by ID\n",
        "                matching_examples = [ex for ex in dataset_dict[lang][base_split] if ex['id'] == example_id]\n",
        "                if matching_examples:\n",
        "                    contexts[lang] = matching_examples[0]['context']\n",
        "                else:\n",
        "                    # Fallback if ID matching fails: use the same index\n",
        "                    contexts[lang] = dataset_dict[lang][base_split][idx]['context']\n",
        "\n",
        "        # Get the original answer based on the configuration\n",
        "        if config['base_dataset'] == config['question_lang']:\n",
        "            # Original answer is from the same dataset as the question\n",
        "            original_answer = example['answers']\n",
        "        else:\n",
        "            # For en_ru_de: we need the answer from the English dataset\n",
        "            matching_examples = [ex for ex in dataset_dict[config['answer_lang']][base_split] if ex['id'] == example_id]\n",
        "            if matching_examples:\n",
        "                original_answer = matching_examples[0]['answers']\n",
        "            else:\n",
        "                original_answer = dataset_dict[config['answer_lang']][base_split][idx]['answers']\n",
        "\n",
        "        # Create chat input with the appropriate configuration\n",
        "        input_text = create_chat_input(question, contexts, config)\n",
        "\n",
        "        # Generate model answer\n",
        "        model_answer = generate_answer(input_text)\n",
        "\n",
        "        # Create result row\n",
        "        result = {\n",
        "            'id': example_id,\n",
        "            f'question_{config[\"question_lang\"]}': question,\n",
        "            'original_answer': original_answer,\n",
        "            'model_answer': model_answer\n",
        "        }\n",
        "\n",
        "        # Save to list\n",
        "        results.append(result)\n",
        "\n",
        "        # Save incrementally to file\n",
        "        with open(output_path, 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        # Create/update CSV file after each batch\n",
        "        if len(results) % batch_size == 0 or idx == total_examples - 1:\n",
        "            pd.DataFrame(results).to_csv(output_csv, index=False)\n",
        "            print(f\"Saved {len(results)} results to CSV\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing example {example_id}: {str(e)}\")\n",
        "        # Save what we have so far\n",
        "        if results:\n",
        "            pd.DataFrame(results).to_csv(output_csv, index=False)\n",
        "\n",
        "# Final DataFrame\n",
        "final_df = pd.DataFrame(results)\n",
        "print(f\"Total processed examples: {len(final_df)}\")\n",
        "\n",
        "# Display the first few rows\n",
        "if len(final_df) > 0:\n",
        "    print(\"\\nSample results:\")\n",
        "    display(final_df.head())"
      ],
      "metadata": {
        "id": "WgnI_jzs8qvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kiKiYt-B_HUT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}