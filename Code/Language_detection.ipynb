{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joykw1/NLP_RAG_project/blob/main/Code/Language_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_P5nM8kSFVX2"
      },
      "outputs": [],
      "source": [
        "input_file = \"/content/multilingual_qa_results_en_all.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLJHlFF5MW_O"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_answers(row):\n",
        "    \"\"\"\n",
        "    Extract text from original_answer dictionary and model_answer text from a single row.\n",
        "\n",
        "    Parameters:\n",
        "    row: A single row from the dataframe (either as Series or dict-like object)\n",
        "\n",
        "    Returns:\n",
        "    tuple: (original_answer_text, model_answer) as strings\n",
        "    \"\"\"\n",
        "    # Extract original answer text using regex\n",
        "    original_answer = str(row['original_answer'])\n",
        "    match = re.search(r\"'text':\\s*\\['([^']+)'\\]\", original_answer)\n",
        "    original_answer_text = match.group(1) if match else \"\"\n",
        "\n",
        "    # Extract model answer directly\n",
        "    model_answer = str(row['model_answer'])\n",
        "\n",
        "    return original_answer_text, model_answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahdmdojvmePU"
      },
      "outputs": [],
      "source": [
        "def is_substring(original_answer_text, model_answer):\n",
        "    \"\"\"\n",
        "    Check if the original_answer_text is a substring of model_answer after preprocessing.\n",
        "\n",
        "    Preprocessing steps:\n",
        "    1) Remove all characters that are not letters or numbers\n",
        "    2) Convert to lowercase\n",
        "\n",
        "    Parameters:\n",
        "    original_answer_text (str): The original answer text\n",
        "    model_answer (str): The model's answer text\n",
        "\n",
        "    Returns:\n",
        "    bool: True if preprocessed original_answer_text is a substring of preprocessed model_answer\n",
        "    \"\"\"\n",
        "    # Preprocessing step 1: Remove all non-alphanumeric characters\n",
        "    clean_original = re.sub(r'[^a-zA-Z0-9а-яА-ЯёЁ]', '', original_answer_text)\n",
        "    clean_model = re.sub(r'[^a-zA-Z0-9а-яА-ЯёЁ]', '', model_answer)\n",
        "\n",
        "    # Preprocessing step 2: Convert to lowercase\n",
        "    clean_original = clean_original.lower()\n",
        "    clean_model = clean_model.lower()\n",
        "\n",
        "    # Check if clean_original is a substring of clean_model\n",
        "    return clean_original in clean_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi7XAkMomhFr"
      },
      "outputs": [],
      "source": [
        "def contains_letters(model_answer):\n",
        "    \"\"\"\n",
        "    Check if the model_answer contains any letters.\n",
        "\n",
        "    Parameters:\n",
        "    model_answer (str): The model's answer text\n",
        "\n",
        "    Returns:\n",
        "    bool: True if the model_answer contains at least one letter, False otherwise\n",
        "    \"\"\"\n",
        "    # Search for any letter (a-z or A-Z) in the string\n",
        "    return bool(re.search('[a-zA-Zа-яА-ЯёЁ]', str(model_answer)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwBVaTgZnYtQ"
      },
      "outputs": [],
      "source": [
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tr50KDCcnO1n"
      },
      "outputs": [],
      "source": [
        "from langdetect import detect\n",
        "\n",
        "def get_language_code(model_answer):\n",
        "    \"\"\"\n",
        "    Detect the language of the model_answer and return its language code.\n",
        "\n",
        "    Parameters:\n",
        "    model_answer (str): The model's answer text\n",
        "\n",
        "    Returns:\n",
        "    str: ISO 639-1 language code (e.g., 'en' for English, 'ru' for Russian)\n",
        "         Returns 'unknown' if detection fails\n",
        "    \"\"\"\n",
        "\n",
        "    # Use langdetect with default return value if it fails\n",
        "    return detect(model_answer) if len(model_answer.strip()) >= 3 else 'unknown'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yfn4WHLkBZiy"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the language detection model once (outside the function)\n",
        "# This is efficient as it avoids reloading the model for each detection\n",
        "language_classifier = pipeline(\"text-classification\",\n",
        "                              model=\"papluca/xlm-roberta-base-language-detection\")\n",
        "\n",
        "def get_language_code_hf(model_answer):\n",
        "    \"\"\"\n",
        "    Detect the language of the model_answer using a Hugging Face model.\n",
        "\n",
        "    Parameters:\n",
        "    model_answer (str): The model's answer text\n",
        "\n",
        "    Returns:\n",
        "    str: ISO 639-1 language code (e.g., 'en' for English, 'ru' for Russian etc)\n",
        "         Returns 'unknown' for empty or invalid inputs\n",
        "    \"\"\"\n",
        "    # Get prediction from the model\n",
        "    result = language_classifier(model_answer)\n",
        "\n",
        "    # Extract the predicted language code (label)\n",
        "    return result[0]['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT8-Uq21Fy7d"
      },
      "outputs": [],
      "source": [
        "!pip install spacy_langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGgYgQSbEQaq"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy_langdetect import LanguageDetector\n",
        "\n",
        "# Load and configure spaCy with language detector once (outside the function)\n",
        "# This only needs to be done once at the beginning of your script\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Load a small model as base\n",
        "\n",
        "# Define factory for language detection\n",
        "@Language.factory(\"language_detector\")\n",
        "def create_language_detector(nlp, name):\n",
        "    return LanguageDetector()\n",
        "\n",
        "# Add the language detector to the pipeline\n",
        "nlp.add_pipe(\"language_detector\", last=True)\n",
        "\n",
        "def get_language_code_spacy(model_answer):\n",
        "    \"\"\"\n",
        "    Detect the language of the model_answer using spaCy and spacy_langdetect.\n",
        "\n",
        "    Parameters:\n",
        "    model_answer (str): The model's answer text\n",
        "\n",
        "    Returns:\n",
        "    str: ISO 639-1 language code (e.g., 'en' for English, 'ru' for Russian)\n",
        "         Returns 'unknown' for empty or invalid inputs\n",
        "    \"\"\"\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(model_answer)\n",
        "\n",
        "    # Get the detected language\n",
        "    language = doc._.language\n",
        "\n",
        "    # Return the language code\n",
        "    return language['language']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zM-nsNOqEztb"
      },
      "outputs": [],
      "source": [
        "def get_majority_language(hf_lang_code, langdetect_code, spacy_lang_code):\n",
        "    \"\"\"\n",
        "    Determine the most common language code among three language detection methods.\n",
        "\n",
        "    Parameters:\n",
        "    hf_lang_code (str): Language code from Hugging Face model\n",
        "    langdetect_code (str): Language code from langdetect library\n",
        "    spacy_lang_code (str): Language code from spaCy detection\n",
        "\n",
        "    Returns:\n",
        "    str: The majority language code, or the first language code if all are different\n",
        "    \"\"\"\n",
        "    # Count occurrences of each language code\n",
        "    lang_codes = [hf_lang_code, langdetect_code, spacy_lang_code]\n",
        "\n",
        "    # Handle 'unknown' values\n",
        "    valid_codes = [code for code in lang_codes if code != 'unknown']\n",
        "\n",
        "    # If all are unknown, return unknown\n",
        "    if not valid_codes:\n",
        "        return 'unknown'\n",
        "\n",
        "    # Count occurrences of each code\n",
        "    code_counts = {}\n",
        "    for code in valid_codes:\n",
        "        if code in code_counts:\n",
        "            code_counts[code] += 1\n",
        "        else:\n",
        "            code_counts[code] = 1\n",
        "\n",
        "    # Find the code with the highest count\n",
        "    majority_code = max(code_counts.items(), key=lambda x: x[1])\n",
        "\n",
        "    # Return the majority code\n",
        "    return majority_code[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxmKuLWbCyDj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "def evaluate_language_answers(question_lang, input_csv_path, output_csv_path=None):\n",
        "    \"\"\"\n",
        "    Process each row in a CSV file, evaluating answers and language detection.\n",
        "\n",
        "    Parameters:\n",
        "    question_lang (str): The language of the question\n",
        "    input_csv_path (str): Path to the input CSV file\n",
        "    output_csv_path (str, optional): Path for the output CSV file.\n",
        "                                    If None, constructs path by adding 'lang_detected_' prefix\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: The processed dataframe with added evaluation columns\n",
        "    \"\"\"\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(input_csv_path)\n",
        "\n",
        "    # Create output path if not provided\n",
        "    if output_csv_path is None:\n",
        "        # Split the path to extract the filename\n",
        "        parts = input_csv_path.split('/')\n",
        "        filename = parts[-1]\n",
        "        # Create output path with prefix\n",
        "        output_csv_path = '/'.join(parts[:-1] + ['lang_detected_' + filename]) if len(parts) > 1 else 'lang_detected_' + filename\n",
        "\n",
        "    # Create new columns for our analysis\n",
        "    df['extracted_original_answer'] = ''\n",
        "    df['has_letters'] = False\n",
        "    df['is_substring'] = False\n",
        "    df['langdetect_code'] = ''\n",
        "    df['hf_lang_code'] = ''\n",
        "    df['spacy_lang_code'] = ''\n",
        "    df['majority_code'] = ''\n",
        "\n",
        "    # Process each row\n",
        "    for index, row in df.iterrows():\n",
        "        # 1) Extract original answer and model answer\n",
        "        original_text, model_text = extract_answers(row)\n",
        "        df.at[index, 'extracted_original_answer'] = original_text\n",
        "\n",
        "        # 2) Check if model answer has letters\n",
        "        has_letters = contains_letters(model_text)\n",
        "        df.at[index, 'has_letters'] = has_letters\n",
        "\n",
        "        # 3) Check if original answer is substring of model answer\n",
        "        is_substring_result = is_substring(original_text, model_text)\n",
        "        df.at[index, 'is_substring'] = is_substring_result\n",
        "\n",
        "        # 4) If model answer has letters, detect language\n",
        "        if is_substring_result:\n",
        "          df.at[index, 'majority_code'] = question_lang\n",
        "        elif has_letters and not is_substring_result:\n",
        "            # Use langdetect\n",
        "            lang_code = get_language_code(model_text)\n",
        "            df.at[index, 'langdetect_code'] = lang_code\n",
        "\n",
        "            # Use Hugging Face model\n",
        "            hf_lang_code = get_language_code_hf(model_text)\n",
        "            df.at[index, 'hf_lang_code'] = hf_lang_code\n",
        "\n",
        "            # Use spaCy\n",
        "            spacy_lang_code = get_language_code_spacy(model_text)\n",
        "            df.at[index, 'spacy_lang_code'] = spacy_lang_code\n",
        "\n",
        "            # Determine the majority language\n",
        "            majority_code = get_majority_language(hf_lang_code, lang_code, spacy_lang_code)\n",
        "            df.at[index, 'majority_code'] = majority_code\n",
        "        elif is_substring:\n",
        "          df.at[index, 'majority_code'] = question_lang\n",
        "        else:\n",
        "          df.at[index, 'majority_code'] = 'no_lang'\n",
        "\n",
        "    # Save the results to a new CSV file\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    # Return the processed dataframe\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IsAOcZrF9MJ"
      },
      "outputs": [],
      "source": [
        "a = evaluate_language_answers(\"en\", input_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OZT4heKGrky"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dNAHpvWGrhx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41kRIeknGret"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIYlthEjGrbm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEzAOJ0IGrYy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9YFcoLmGrV9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX3J3LPnGrTV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji2odsxBGrQo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2QEHVHGGrOI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-78Ki70PGCGD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import os\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "def extraction_filename (csv_path):\n",
        "  # Extract filename without path\n",
        "  filename = os.path.basename(csv_path)  # \"lang_detected_multilingual_qa_results_en_all.csv\"\n",
        "\n",
        "  # Remove extension and split by `_`\n",
        "  parts = filename.replace('.csv', '').split('_')\n",
        "\n",
        "  print(f\"Parts {parts}\")\n",
        "\n",
        "  # Extract the last part (i.e., \"en_all\")\n",
        "  if \"all\" in parts[-1]:return '_'.join(parts[-2:])\n",
        "  else: return '_'.join(parts[-3:])\n",
        "\n",
        "\n",
        "\n",
        "def extraction_plot_titles(csv_path, language_map):\n",
        "  # Remove extension and split by `_`\n",
        "  parts = csv_path.replace('.csv', '').split('_')\n",
        "\n",
        "  print(f\"Parts {parts}\")\n",
        "\n",
        "\n",
        "  # Extract the last part (i.e., \"en_all\")\n",
        "  if \"all\" in parts[-1]:\n",
        "    return f'Question: {language_map[parts[-2]]}, Context: English, German and Russian'\n",
        "  else:\n",
        "    return f'Question: {language_map[parts[-3]]}, Context: {language_map[parts[-2]]} and {language_map[parts[-1]]}'\n",
        "\n",
        "\n",
        "\n",
        "def visualize_language_distribution(csv_path, output_prefix=\"language_viz\"):\n",
        "    \"\"\"\n",
        "    Visualize language distribution and substring matching from the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    csv_path (str): Path to the CSV file with language detection results\n",
        "    output_prefix (str): Prefix for output image filenames\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Convert boolean columns if they're stored as strings\n",
        "    if df['is_substring'].dtype == 'object':\n",
        "        df['is_substring'] = df['is_substring'].map({'True': True, 'False': False})\n",
        "    if df['has_letters'].dtype == 'object':\n",
        "        df['has_letters'] = df['has_letters'].map({'True': True, 'False': False})\n",
        "\n",
        "    # Create a language mapping for better readability\n",
        "    language_map = {\n",
        "        'en': 'English', 'de': 'German', 'ru': 'Russian', 'fr': 'French',\n",
        "        'it': 'Italian', 'es': 'Spanish', 'pt': 'Portuguese', 'da': 'Danish',\n",
        "        'so': 'Somali', 'ca': 'Catalan', 'sw': 'Swahili', 'ro': 'Romanian',\n",
        "        'pl': 'Polish', 'no': 'Norwegian', 'sv': 'Swedish', 'vi': 'Vietnamese',\n",
        "        'fi': 'Finnish', 'id': 'Indonesian', 'tl': 'Tagalog', 'et': 'Estonian',\n",
        "        'sl': 'Slovenian', 'nl': 'Dutch', 'cs': 'Czech', 'af': 'Afrikaans',\n",
        "        'ur': 'Urdu', 'lt': 'Lithuanian', 'lv': 'Latvian', 'hr': 'Croatian',\n",
        "        'bg': 'Bulgarian', 'mk': 'Macedonian', 'uk': 'Ukrainian', 'tr' : 'Turkish'\n",
        "    }\n",
        "\n",
        "    # Replace language codes with full names where possible\n",
        "    df['language_name'] = df['majority_code'].map(language_map).fillna(df['majority_code'])\n",
        "\n",
        "    # Count language occurrences and sort by frequency\n",
        "    lang_counts = df['language_name'].value_counts()\n",
        "\n",
        "    # Calculate substring matching by language\n",
        "    lang_substring = df.groupby(['language_name', 'is_substring']).size().unstack(fill_value=0)\n",
        "    if True not in lang_substring.columns:\n",
        "        lang_substring[True] = 0\n",
        "    if False not in lang_substring.columns:\n",
        "        lang_substring[False] = 0\n",
        "\n",
        "    lang_substring = lang_substring.reset_index()\n",
        "    lang_substring.columns = ['Language', 'No Match', 'Match']\n",
        "    lang_substring['Total'] = lang_substring['Match'] + lang_substring['No Match']\n",
        "    lang_substring['Match Rate'] = (lang_substring['Match'] / lang_substring['Total'] * 100).round(2)\n",
        "\n",
        "    # Sort by total count\n",
        "    lang_substring = lang_substring.sort_values('Total', ascending=False)\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"=== Summary Statistics ===\")\n",
        "    print(f\"Total languages: {len(lang_counts)}\")\n",
        "    print(f\"Total answers: {len(df)}\")\n",
        "    print(f\"Substring matches: {df['is_substring'].sum()} ({df['is_substring'].mean()*100:.2f}%)\")\n",
        "    print(f\"No matches: {len(df) - df['is_substring'].sum()}\")\n",
        "    print(\"\\n=== Top 10 Languages ===\")\n",
        "    print(lang_substring[['Language', 'Total', 'Match', 'No Match', 'Match Rate']].head(10).to_string(index=False))\n",
        "\n",
        "    ### Visualizations\n",
        "    \n",
        "    #output_folder = \"/content/drive/My Drive/NLP_final_project_lang_detected/\"\n",
        "    output_prefix = extraction_filename(csv_path)\n",
        "    title_lang = extraction_plot_titles(csv_path, language_map)\n",
        "    \n",
        "\n",
        "    ## 1. Bar chart of top languages\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    top_n = 15  # Show top N languages\n",
        "\n",
        "    # Get top N languages\n",
        "    top_langs = lang_substring.head(top_n)\n",
        "\n",
        "    # Create stacked bar chart\n",
        "    ax = plt.subplot(111)\n",
        "    top_langs.plot(x='Language', y=['Match', 'No Match'], kind='bar', stacked=True,\n",
        "                  color=['#4CAF50', '#F44336'], ax=ax)\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.title(f'Top Languages by Frequency and Substring Matching. {title_lang}.', fontsize=16)\n",
        "    plt.xlabel('Language', fontsize=14)\n",
        "    plt.ylabel('Number of Answers', fontsize=14)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Add totals on top of bars\n",
        "    for i, total in enumerate(top_langs['Total']):\n",
        "        plt.text(i, total + 0.5, str(total), ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "    # Tight layout and save figure\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_prefix}_bar_chart.png\", dpi=300)\n",
        "    #plt.savefig(os.path.join(output_folder, f\"{output_prefix}_bar_chart.png\"), dpi=300)\n",
        "\n",
        "\n",
        "\n",
        "    ## 2. Pie chart of language distribution\n",
        "    plt.figure(figsize=(12, 12))\n",
        "\n",
        "    # Group smaller languages as \"Other\"\n",
        "    threshold = 5  # Languages with fewer than this count will be grouped\n",
        "    other_langs = lang_counts[lang_counts < threshold].sum()\n",
        "    major_langs = lang_counts[lang_counts >= threshold]\n",
        "\n",
        "    if other_langs > 0:\n",
        "        pie_data = pd.concat([major_langs, pd.Series({'Other': other_langs})])\n",
        "    else:\n",
        "        pie_data = major_langs\n",
        "\n",
        "\n",
        "    # Prepare the data for Plotly\n",
        "    pie_data_df = pd.DataFrame(pie_data).reset_index()\n",
        "    pie_data_df.columns = ['Language', 'Count']\n",
        "\n",
        "    # Create a pie chart using Plotly Express\n",
        "    fig = px.pie(pie_data_df,\n",
        "                values='Count',\n",
        "                names='Language',\n",
        "                title='Distribution of Languages',\n",
        "                hover_data={'Language': True, 'Count': True},\n",
        "                labels={'Count': 'Count'},  # Rename \"Count\" label in the legend\n",
        "                hole=0.3)  # This creates a \"donut\" chart if needed\n",
        "\n",
        "    # Customize the pie chart to show percentages inside, labels outside, and add legend with counts\n",
        "    fig.update_traces(textposition='outside',\n",
        "                      textinfo='percent+label',\n",
        "                      texttemplate='%{label} %{percent:.1%}',  # Round percentages to whole numbers\n",
        "                      hoverinfo='label+percent+value',\n",
        "                      marker=dict(line=dict(color='white', width=2)),  # White border between slices\n",
        "                      showlegend=True)\n",
        "\n",
        "    # Customizing legend labels to include the count next to the language\n",
        "    legend_labels = {label: f\"{label} ({count})\" for label, count in zip(pie_data_df['Language'], pie_data_df['Count'])}\n",
        "    print(fig.data)\n",
        "\n",
        "    # Updating the labels in the figure\n",
        "    fig.data[0].labels = [legend_labels[label] for label in fig.data[0].labels]\n",
        "\n",
        "\n",
        "\n",
        "    # Adjust the layout for the title and legend\n",
        "    fig.update_layout(\n",
        "        title=f'Distribution of Languages.\\n{title_lang}',  # Change title as needed\n",
        "        showlegend=True,\n",
        "        legend_title='Languages (Count)',\n",
        "        width=1000,  # Set figure width\n",
        "        height=1000,  # Set figure height\n",
        "        title_x=0.5,  # Center the title (0 = left, 1 = right)\n",
        "        title_y=0.05  # Move title closer or further from the plot\n",
        "    )\n",
        "\n",
        "    # Save the image as html file\n",
        "    #fig.write_html(os.path.join(output_folder, f\"{output_prefix}_pie_chart_plotly.html\"))\n",
        "    fig.write_html(f\"{output_prefix}_pie_chart_plotly.html\")\n",
        "\n",
        "    \n",
        "\n",
        "    # Show the plot\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Show summary message\n",
        "    print(f\"\\nVisualization complete! Files saved with prefix '{output_prefix}'\")\n",
        "    return lang_substring\n",
        "\n",
        "# Example usage:\n",
        "language_data = visualize_language_distribution('lang_detected_multilingual_qa_results_en_ru_de.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM2AGdc0LdX4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNfO9AStUIwlZoVaEmXK7Ot",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
