{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUFLecfpc35k"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets//colab-badge.svg)](https://colab.research.google.com/github/Joykw1/NLP_RAG_project/blob/main/Code/Language_detection.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"/content/multilingual_qa_results_en_all.csv\""
      ],
      "metadata": {
        "id": "_P5nM8kSFVX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_answers(row):\n",
        "    \"\"\"\n",
        "    Extract text from original_answer dictionary and model_answer text from a single row.\n",
        "\n",
        "    Parameters:\n",
        "    row: A single row from the dataframe (either as Series or dict-like object)\n",
        "\n",
        "    Returns:\n",
        "    tuple: (original_answer_text, model_answer) as strings\n",
        "    \"\"\"\n",
        "    # Extract original answer text using regex\n",
        "    original_answer = str(row['original_answer'])\n",
        "    match = re.search(r\"'text':\\s*\\['([^']+)'\\]\", original_answer)\n",
        "    original_answer_text = match.group(1) if match else \"\"\n",
        "\n",
        "    # Extract model answer directly\n",
        "    model_answer = str(row['model_answer'])\n",
        "\n",
        "    return original_answer_text, model_answer\n",
        "\n"
      ],
      "metadata": {
        "id": "RLJHlFF5MW_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_substring(original_answer_text, model_answer):\n",
        "    \"\"\"\n",
        "    Check if the original_answer_text is a substring of model_answer after preprocessing.\n",
        "\n",
        "    Preprocessing steps:\n",
        "    1) Remove all characters that are not letters or numbers\n",
        "    2) Convert to lowercase\n",
        "\n",
        "    Parameters:\n",
        "    original_answer_text (str): The original answer text\n",
        "    model_answer (str): The model's answer text\n",
        "\n",
        "    Returns:\n",
        "    bool: True if preprocessed original_answer_text is a substring of preprocessed model_answer\n",
        "    \"\"\"\n",
        "    # Preprocessing step 1: Remove all non-alphanumeric characters\n",
        "    clean_original = re.sub(r'[^a-zA-Z0-9а-яА-ЯёЁ]', '', original_answer_text)\n",
        "    clean_model = re.sub(r'[^a-zA-Z0-9а-яА-ЯёЁ]', '', model_answer)\n",
        "\n",
        "    # Preprocessing step 2: Convert to lowercase\n",
        "    clean_original = clean_original.lower()\n",
        "    clean_model = clean_model.lower()\n",
        "\n",
        "    # Check if clean_original is a substring of clean_model\n",
        "    return clean_original in clean_model"
      ],
      "metadata": {
        "id": "ahdmdojvmePU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contains_letters(model_answer):\n",
        "    \"\"\"\n",
        "    Check if the model_answer contains any letters.\n",
        "\n",
        "    Parameters:\n",
        "    model_answer (str): The model's answer text\n",
        "\n",
        "    Returns:\n",
        "    bool: True if the model_answer contains at least one letter, False otherwise\n",
        "    \"\"\"\n",
        "    # Search for any letter (a-z or A-Z) in the string\n",
        "    return bool(re.search('[a-zA-Zа-яА-ЯёЁ]', str(model_answer)))"
      ],
      "metadata": {
        "id": "yi7XAkMomhFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwBVaTgZnYtQ",
        "outputId": "db3e261b-2217-470c-875d-437b5cc13499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect\n",
        "\n",
        "def get_language_code(model_answer):\n",
        "    \"\"\"\n",
        "    Detect the language of the model_answer and return its language code.\n",
        "\n",
        "    Parameters:\n",
        "    model_answer (str): The model's answer text\n",
        "\n",
        "    Returns:\n",
        "    str: ISO 639-1 language code (e.g., 'en' for English, 'ru' for Russian)\n",
        "         Returns 'unknown' if detection fails\n",
        "    \"\"\"\n",
        "\n",
        "    # Use langdetect with default return value if it fails\n",
        "    return detect(model_answer) if len(model_answer.strip()) >= 3 else 'unknown'"
      ],
      "metadata": {
        "id": "tr50KDCcnO1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the language detection model once (outside the function)\n",
        "# This is efficient as it avoids reloading the model for each detection\n",
        "language_classifier = pipeline(\"text-classification\",\n",
        "                              model=\"papluca/xlm-roberta-base-language-detection\")\n",
        "\n",
        "def get_language_code_hf(model_answer):\n",
        "    \"\"\"\n",
        "    Detect the language of the model_answer using a Hugging Face model.\n",
        "\n",
        "    Parameters:\n",
        "    model_answer (str): The model's answer text\n",
        "\n",
        "    Returns:\n",
        "    str: ISO 639-1 language code (e.g., 'en' for English, 'ru' for Russian etc)\n",
        "         Returns 'unknown' for empty or invalid inputs\n",
        "    \"\"\"\n",
        "    # Get prediction from the model\n",
        "    result = language_classifier(model_answer)\n",
        "\n",
        "    # Extract the predicted language code (label)\n",
        "    return result[0]['label']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yfn4WHLkBZiy",
        "outputId": "e0ff085a-1bab-45e2-9e9c-0120eb16bc38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy_langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT8-Uq21Fy7d",
        "outputId": "49dcc0c2-edd5-402e-af15-0f48fa975845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy_langdetect in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from spacy_langdetect) (8.3.5)\n",
            "Requirement already satisfied: langdetect==1.0.7 in /usr/local/lib/python3.11/dist-packages (from spacy_langdetect) (1.0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect==1.0.7->spacy_langdetect) (1.17.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->spacy_langdetect) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest->spacy_langdetect) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->spacy_langdetect) (1.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy_langdetect import LanguageDetector\n",
        "\n",
        "# Load and configure spaCy with language detector once (outside the function)\n",
        "# This only needs to be done once at the beginning of your script\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Load a small model as base\n",
        "\n",
        "# Define factory for language detection\n",
        "@Language.factory(\"language_detector\")\n",
        "def create_language_detector(nlp, name):\n",
        "    return LanguageDetector()\n",
        "\n",
        "# Add the language detector to the pipeline\n",
        "nlp.add_pipe(\"language_detector\", last=True)\n",
        "\n",
        "def get_language_code_spacy(model_answer):\n",
        "    \"\"\"\n",
        "    Detect the language of the model_answer using spaCy and spacy_langdetect.\n",
        "\n",
        "    Parameters:\n",
        "    model_answer (str): The model's answer text\n",
        "\n",
        "    Returns:\n",
        "    str: ISO 639-1 language code (e.g., 'en' for English, 'ru' for Russian)\n",
        "         Returns 'unknown' for empty or invalid inputs\n",
        "    \"\"\"\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(model_answer)\n",
        "\n",
        "    # Get the detected language\n",
        "    language = doc._.language\n",
        "\n",
        "    # Return the language code\n",
        "    return language['language']"
      ],
      "metadata": {
        "id": "kGgYgQSbEQaq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "51143b21-18e4-4d51-9e72-497d9dfcd5fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E004] Can't set up pipeline component: a factory for 'language_detector' already exists. Existing factory: <function create_language_detector at 0x7eb7761dc720>. New factory: <function create_language_detector at 0x7eb7610d3ba0>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-64220bad2af8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Define factory for language detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"language_detector\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_language_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mLanguageDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_factory\u001b[0;34m(factory_func)\u001b[0m\n\u001b[1;32m    519\u001b[0m                         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexisting_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfactory_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                     )\n\u001b[0;32m--> 521\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_arg_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E004] Can't set up pipeline component: a factory for 'language_detector' already exists. Existing factory: <function create_language_detector at 0x7eb7761dc720>. New factory: <function create_language_detector at 0x7eb7610d3ba0>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_majority_language(hf_lang_code, langdetect_code, spacy_lang_code):\n",
        "    \"\"\"\n",
        "    Determine the most common language code among three language detection methods.\n",
        "\n",
        "    Parameters:\n",
        "    hf_lang_code (str): Language code from Hugging Face model\n",
        "    langdetect_code (str): Language code from langdetect library\n",
        "    spacy_lang_code (str): Language code from spaCy detection\n",
        "\n",
        "    Returns:\n",
        "    str: The majority language code, or the first language code if all are different\n",
        "    \"\"\"\n",
        "    # Count occurrences of each language code\n",
        "    lang_codes = [hf_lang_code, langdetect_code, spacy_lang_code]\n",
        "\n",
        "    # Handle 'unknown' values\n",
        "    valid_codes = [code for code in lang_codes if code != 'unknown']\n",
        "\n",
        "    # If all are unknown, return unknown\n",
        "    if not valid_codes:\n",
        "        return 'unknown'\n",
        "\n",
        "    # Count occurrences of each code\n",
        "    code_counts = {}\n",
        "    for code in valid_codes:\n",
        "        if code in code_counts:\n",
        "            code_counts[code] += 1\n",
        "        else:\n",
        "            code_counts[code] = 1\n",
        "\n",
        "    # Find the code with the highest count\n",
        "    majority_code = max(code_counts.items(), key=lambda x: x[1])\n",
        "\n",
        "    # Return the majority code\n",
        "    return majority_code[0]"
      ],
      "metadata": {
        "id": "zM-nsNOqEztb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "def evaluate_language_answers(question_lang, input_csv_path, output_csv_path=None):\n",
        "    \"\"\"\n",
        "    Process each row in a CSV file, evaluating answers and language detection.\n",
        "\n",
        "    Parameters:\n",
        "    question_lang (str): The language of the question\n",
        "    input_csv_path (str): Path to the input CSV file\n",
        "    output_csv_path (str, optional): Path for the output CSV file.\n",
        "                                    If None, constructs path by adding 'lang_detected_' prefix\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: The processed dataframe with added evaluation columns\n",
        "    \"\"\"\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(input_csv_path)\n",
        "\n",
        "    # Create output path if not provided\n",
        "    if output_csv_path is None:\n",
        "        # Split the path to extract the filename\n",
        "        parts = input_csv_path.split('/')\n",
        "        filename = parts[-1]\n",
        "        # Create output path with prefix\n",
        "        output_csv_path = '/'.join(parts[:-1] + ['lang_detected_' + filename]) if len(parts) > 1 else 'lang_detected_' + filename\n",
        "\n",
        "    # Create new columns for our analysis\n",
        "    df['extracted_original_answer'] = ''\n",
        "    df['has_letters'] = False\n",
        "    df['is_substring'] = False\n",
        "    df['langdetect_code'] = ''\n",
        "    df['hf_lang_code'] = ''\n",
        "    df['spacy_lang_code'] = ''\n",
        "    df['majority_code'] = ''\n",
        "\n",
        "    # Process each row\n",
        "    for index, row in df.iterrows():\n",
        "        # 1) Extract original answer and model answer\n",
        "        original_text, model_text = extract_answers(row)\n",
        "        df.at[index, 'extracted_original_answer'] = original_text\n",
        "\n",
        "        # 2) Check if model answer has letters\n",
        "        has_letters = contains_letters(model_text)\n",
        "        df.at[index, 'has_letters'] = has_letters\n",
        "\n",
        "        # 3) Check if original answer is substring of model answer\n",
        "        is_substring_result = is_substring(original_text, model_text)\n",
        "        df.at[index, 'is_substring'] = is_substring_result\n",
        "\n",
        "        # 4) If model answer has letters, detect language\n",
        "        if is_substring_result:\n",
        "          df.at[index, 'majority_code'] = question_lang\n",
        "        elif has_letters and not is_substring_result:\n",
        "            # Use langdetect\n",
        "            lang_code = get_language_code(model_text)\n",
        "            df.at[index, 'langdetect_code'] = lang_code\n",
        "\n",
        "            # Use Hugging Face model\n",
        "            hf_lang_code = get_language_code_hf(model_text)\n",
        "            df.at[index, 'hf_lang_code'] = hf_lang_code\n",
        "\n",
        "            # Use spaCy\n",
        "            spacy_lang_code = get_language_code_spacy(model_text)\n",
        "            df.at[index, 'spacy_lang_code'] = spacy_lang_code\n",
        "\n",
        "            # Determine the majority language\n",
        "            majority_code = get_majority_language(hf_lang_code, lang_code, spacy_lang_code)\n",
        "            df.at[index, 'majority_code'] = majority_code\n",
        "        elif is_substring:\n",
        "          df.at[index, 'majority_code'] = question_lang\n",
        "        else:\n",
        "          df.at[index, 'majority_code'] = 'no_lang'\n",
        "\n",
        "    # Save the results to a new CSV file\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    # Return the processed dataframe\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "GxmKuLWbCyDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = evaluate_language_answers(\"en\", input_file)"
      ],
      "metadata": {
        "id": "3IsAOcZrF9MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2OZT4heKGrky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-dNAHpvWGrhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "41kRIeknGret"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HIYlthEjGrbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bEzAOJ0IGrYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k9YFcoLmGrV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bX3J3LPnGrTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ji2odsxBGrQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t2QEHVHGGrOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def visualize_language_distribution(csv_path, output_prefix=\"language_viz\"):\n",
        "    \"\"\"\n",
        "    Visualize language distribution and substring matching from the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    csv_path (str): Path to the CSV file with language detection results\n",
        "    output_prefix (str): Prefix for output image filenames\n",
        "    \"\"\"\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Convert boolean columns if they're stored as strings\n",
        "    if df['is_substring'].dtype == 'object':\n",
        "        df['is_substring'] = df['is_substring'].map({'True': True, 'False': False})\n",
        "    if df['has_letters'].dtype == 'object':\n",
        "        df['has_letters'] = df['has_letters'].map({'True': True, 'False': False})\n",
        "\n",
        "    # Create a language mapping for better readability\n",
        "    language_map = {\n",
        "        'en': 'English', 'de': 'German', 'ru': 'Russian', 'fr': 'French',\n",
        "        'it': 'Italian', 'es': 'Spanish', 'pt': 'Portuguese', 'da': 'Danish',\n",
        "        'so': 'Somali', 'ca': 'Catalan', 'sw': 'Swahili', 'ro': 'Romanian',\n",
        "        'pl': 'Polish', 'no': 'Norwegian', 'sv': 'Swedish', 'vi': 'Vietnamese',\n",
        "        'fi': 'Finnish', 'id': 'Indonesian', 'tl': 'Tagalog', 'et': 'Estonian',\n",
        "        'sl': 'Slovenian', 'nl': 'Dutch', 'cs': 'Czech', 'af': 'Afrikaans',\n",
        "        'ur': 'Urdu', 'lt': 'Lithuanian', 'lv': 'Latvian', 'hr': 'Croatian'\n",
        "    }\n",
        "\n",
        "    # Replace language codes with full names where possible\n",
        "    df['language_name'] = df['majority_code'].map(language_map).fillna(df['majority_code'])\n",
        "\n",
        "    # Count language occurrences and sort by frequency\n",
        "    lang_counts = df['language_name'].value_counts()\n",
        "\n",
        "    # Calculate substring matching by language\n",
        "    lang_substring = df.groupby(['language_name', 'is_substring']).size().unstack(fill_value=0)\n",
        "    if True not in lang_substring.columns:\n",
        "        lang_substring[True] = 0\n",
        "    if False not in lang_substring.columns:\n",
        "        lang_substring[False] = 0\n",
        "\n",
        "    lang_substring = lang_substring.reset_index()\n",
        "    lang_substring.columns = ['Language', 'No Match', 'Match']\n",
        "    lang_substring['Total'] = lang_substring['Match'] + lang_substring['No Match']\n",
        "    lang_substring['Match Rate'] = (lang_substring['Match'] / lang_substring['Total'] * 100).round(2)\n",
        "\n",
        "    # Sort by total count\n",
        "    lang_substring = lang_substring.sort_values('Total', ascending=False)\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"=== Summary Statistics ===\")\n",
        "    print(f\"Total languages: {len(lang_counts)}\")\n",
        "    print(f\"Total answers: {len(df)}\")\n",
        "    print(f\"Substring matches: {df['is_substring'].sum()} ({df['is_substring'].mean()*100:.2f}%)\")\n",
        "    print(f\"No matches: {len(df) - df['is_substring'].sum()}\")\n",
        "    print(\"\\n=== Top 10 Languages ===\")\n",
        "    print(lang_substring[['Language', 'Total', 'Match', 'No Match', 'Match Rate']].head(10).to_string(index=False))\n",
        "\n",
        "    # Visualizations\n",
        "    # 1. Bar chart of top languages\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    top_n = 15  # Show top N languages\n",
        "\n",
        "    # Get top N languages\n",
        "    top_langs = lang_substring.head(top_n)\n",
        "\n",
        "    # Create stacked bar chart\n",
        "    ax = plt.subplot(111)\n",
        "    top_langs.plot(x='Language', y=['Match', 'No Match'], kind='bar', stacked=True,\n",
        "                  color=['#4CAF50', '#F44336'], ax=ax)\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.title('Top Languages by Frequency and Substring Matching', fontsize=16)\n",
        "    plt.xlabel('Language', fontsize=14)\n",
        "    plt.ylabel('Number of Answers', fontsize=14)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Add totals on top of bars\n",
        "    for i, total in enumerate(top_langs['Total']):\n",
        "        plt.text(i, total + 0.5, str(total), ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "    # Tight layout and save figure\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_prefix}_bar_chart.png\", dpi=300)\n",
        "\n",
        "    # 2. Pie chart of language distribution\n",
        "    plt.figure(figsize=(12, 12))\n",
        "\n",
        "    # Group smaller languages as \"Other\"\n",
        "    threshold = 5  # Languages with fewer than this count will be grouped\n",
        "    other_langs = lang_counts[lang_counts < threshold].sum()\n",
        "    major_langs = lang_counts[lang_counts >= threshold]\n",
        "\n",
        "    if other_langs > 0:\n",
        "        pie_data = pd.concat([major_langs, pd.Series({'Other': other_langs})])\n",
        "    else:\n",
        "        pie_data = major_langs\n",
        "\n",
        "    # Create pie chart\n",
        "    patches, texts, autotexts = plt.pie(\n",
        "        pie_data,\n",
        "        labels=pie_data.index,\n",
        "        autopct='%1.1f%%',\n",
        "        startangle=90,\n",
        "        shadow=False,\n",
        "        explode=[0.05] * len(pie_data),  # Slight separation for all slices\n",
        "        textprops={'fontsize': 12}\n",
        "    )\n",
        "\n",
        "    # Equal aspect ratio ensures that pie is drawn as a circle\n",
        "    plt.axis('equal')\n",
        "    plt.title('Distribution of Languages', fontsize=16)\n",
        "\n",
        "    # Add legend with counts\n",
        "    legend_labels = [f\"{label} ({count})\" for label, count in zip(pie_data.index, pie_data)]\n",
        "    plt.legend(patches, legend_labels, loc=\"best\", frameon=True, fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_prefix}_pie_chart.png\", dpi=300)\n",
        "\n",
        "\n",
        "\n",
        "    # Show summary message\n",
        "    print(f\"\\nVisualization complete! Files saved with prefix '{output_prefix}'\")\n",
        "    return lang_substring\n",
        "\n",
        "# Example usage:\n",
        "language_data = visualize_language_distribution('lang_detected_multilingual_qa_results_en_ru_de.csv')"
      ],
      "metadata": {
        "id": "-78Ki70PGCGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XM2AGdc0LdX4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
